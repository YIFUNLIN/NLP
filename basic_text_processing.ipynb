{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=8, micro=13, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import wget\n",
    "from collections import Counter  #計算字的次數，用來作詞頻分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUNCT_TO_REMOVE: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "remove_punct_text: Artificial Intelligence AI sometimes called machine intelligence we learning it\n"
     ]
    }
   ],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation   #內建會移除的標點符號\n",
    "print(\"PUNCT_TO_REMOVE:\",PUNCT_TO_REMOVE)\n",
    "\n",
    "text = \"Artificial Intelligence (AI), sometimes called machine intelligence, we learning it\"\n",
    "remove_punct_text = text.translate(str.maketrans(\"\",\"\",PUNCT_TO_REMOVE))  #將標點符號用空白取代\n",
    "print(\"remove_punct_text:\",remove_punct_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove of Stopwords (代詞、代名詞那些會被移除)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOPWORDS: i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\n",
      "remove_stopword_text: Artificial Intelligence (AI), sometimes called machine intelligence, learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yifun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##下載 stoppwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words('english')\n",
    "print(\"STOPWORDS:\",\", \".join(STOPWORDS))\n",
    "\n",
    "text = \"Artificial Intelligence (AI), sometimes called machine intelligence, we learning it\"\n",
    "remove_stopword_text = [word for word in text.split() if word not in STOPWORDS]  #先將句子切割成單詞， 如果不是stopwords就取出來\n",
    "print(\"remove_stopword_text:\", \" \".join(remove_stopword_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove of Frequent words (移除高頻字)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter: Counter({'Artificial': 2, 'Intelligence': 2, '(AI),': 1, 'sometimes': 1, 'called': 1, 'machine': 1, 'intelligence': 1, 'was': 1, 'founded': 1, 'as': 1, 'an': 1, 'academic': 1, 'displine': 1, 'in': 1, '1995': 1})\n",
      "FREQWORDS: {'(AI),', 'Intelligence', 'Artificial'}\n",
      "remove_fre_text: [['sometimes', 'called', 'machine', 'intelligence'], ['was', 'founded', 'as', 'an', 'academic', 'displine', 'in', '1995']]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "num = 3\n",
    "text = [\"Artificial Intelligence (AI), sometimes called machine intelligence\".split(),\n",
    "        \"Artificial Intelligence was founded as an academic displine in 1995\".split()]  #雙層的list\n",
    "\n",
    "cnt = Counter([word for sent in text for word in sent])  #從兩層的list中先取出每一句話，再從每一句話中取出每個單詞，再利用Counter做累積次數\n",
    "print(\"Counter:\",cnt)\n",
    "\n",
    "FREQWORDS = set([w for (w,wc) in cnt.most_common(num)]) #利用most_common()去讀出出現次數最高的前幾名的詞，(w,wc)代表該單詞與單詞出現的次數\n",
    "print(\"FREQWORDS:\",FREQWORDS)\n",
    "\n",
    "remove_fre_text = [[word for word in sent if word not in FREQWORDS] for sent in text]   #從右往左看回去，先取出每個句子，再取出句子中的每個詞，如果不是高頻詞才會抓出來\n",
    "print(\"remove_fre_text:\",remove_fre_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove of Rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RARWORDS: ['as', 'an', 'academic', 'displine', 'in']\n",
      "remove_rare_text: [['Artificial', 'Intelligence', '(AI),', 'sometimes', 'called', 'machine', 'intelligence'], ['Artificial', 'Intelligence', 'was', 'founded', '1995']]\n"
     ]
    }
   ],
   "source": [
    "num = 5\n",
    "text = [\"Artificial Intelligence (AI), sometimes called machine intelligence\".split(),\n",
    "        \"Artificial Intelligence was founded as an academic displine in 1995\".split()]  #雙層的list\n",
    "\n",
    "cnt = Counter([word for sent in text for word in sent])\n",
    "\n",
    "RARWORDS = [w for (w, wc) in cnt.most_common()[-num-1:-1]]\n",
    "print(\"RARWORDS:\",RARWORDS)\n",
    "\n",
    "remove_rare_text = [[word for word in sent if word not in RARWORDS]for sent in text]\n",
    "print(\"remove_rare_text:\",remove_rare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming 字根還原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmer:  <PorterStemmer>\n",
      "srem_text:  artifici intellig (ai), sometim call machin intelligence, we learn it\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem .porter import PorterStemmer\n",
    "stemmer = PorterStemmer()  #使用Porter Algo 就可以去創造物件\n",
    "print(\"stemmer: \", stemmer)\n",
    "\n",
    "text = \"Artificial Intelligence (AI), sometimes called machine intelligence, we learning it\"\n",
    "stem_text = \" \".join([stemmer.stem(word) for word in text.split()])   #針對一句話的每個字做還原字根\n",
    "print(\"srem_text: \", stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization 詞形還原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_text: Artificial Intelligence wa founded a an academic displine in 1995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yifun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\yifun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer =  WordNetLemmatizer()\n",
    "\n",
    "text = \"Artificial Intelligence was founded as an academic displine in 1995\"\n",
    "lemma_text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "print(\"lemma_text:\",lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove_url_text: Artificial Intelligence was founded as an academic displine in 1995 at \n"
     ]
    }
   ],
   "source": [
    "url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "\n",
    "text = \"Artificial Intelligence was founded as an academic displine in 1995 at https://zh.wikipedia.org/zh-tw/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD\"\n",
    "\n",
    "remove_url_text = url_pattern.sub(r'',text)\n",
    "print(\"Remove_url_text:\",remove_url_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_text = ['人工智慧亦稱智械、機器智慧,指由人製造出來的機器所表現出来的智慧。',\n",
    "'通常人工智慧是指透過普通電腦程式來现人题智慧的技術。',\n",
    "'中華郵政未来智慧物流服務,將取之大眾智慧,中華郵政帶給民眾更好的便利生活。']\n",
    "\n",
    "en_text = ['In 1951, a board of directors was created and premises were rented on Hankou Street in downtown Taipe; to set up the so-called Soochow Preparatory Schoo.',\n",
    "'The school became the first private university in Taiwan. ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jieba - Chinese text segmentation\n",
    "\n",
    "使用說明: https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg  #用來做詞性標註的\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from c:\\Users\\yifun\\Desktop\\python_code\\dict.txt.big ...\n",
      "Dumping model to file cache C:\\Users\\yifun\\AppData\\Local\\Temp\\jieba.uea161c6b71afd61d5734afab1a200174.cache\n",
      "Loading model cost 1.811 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#採用平行運算\n",
    "#僅支援 jieba.dt和 jieba.pogseg.dt\n",
    "# Windows不支援\n",
    "# 例如使用4個核心執行\n",
    "#i.e.\n",
    "#jieba.enable parallel(4)\n",
    "\n",
    "#下載繁體中文字典檔和自定義詞典檔\n",
    "# 下載繁體中文字典檔和自定義詞典檔\n",
    "wget.download('https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big', 'dict.txt.big')\n",
    "wget.download('https://raw.githubusercontent.com/fxsjy/jieba/master/test/userdict.txt', 'userdict.txt')\n",
    "\n",
    "# 載入指定主詞典路徑\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "\n",
    "# 載入指定自定義詞典路徑\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "# 如果不是在 Windows 平台，且想要使用平行運算，可以取消下一行的註解\n",
    "# jieba.enable_parallel(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Paddle 模式]\n",
      "[['人工智慧', '亦', '稱智械', '、', '機器', '智慧', ',', '指由人', '製造', '出來', '的', '機器', '所', '表現', '出来', '的', '智慧', '。'], ['通常', '人工智慧', '是', '指', '透過', '普通', '電腦程式', '來现', '人题', '智慧', '的', '技術', '。'], ['中華', '郵政', '未来', '智慧', '物流', '服務', ',', '將取', '之大眾', '智慧', ',', '中華', '郵政', '帶給', '民眾', '更好', '的', '便利', '生活', '。']]\n",
      "\n",
      " [全模式]\n",
      "[['人工', '人工智慧', '智慧', '亦', '稱', '智', '械', '、', '機器', '智慧', ',', '指', '由', '人', '製造', '造出', '造出來', '出來', '的', '機器', '所', '表現', '現出', '出来', '的', '智慧', '。'], ['通常', '常人', '人工', '人工智慧', '智慧', '是', '指', '透過', '普通', '通電', '電腦', '電腦程式', '程式', '來', '现', '人', '题', '智慧', '的', '技術', '。'], ['中華', '中華郵', '郵政', '未来', '智慧', '物流', '服務', ',', '將', '取', '之', '大', '眾', '智慧', ',', '中華', '中華郵', '郵政', '帶給', '民', '眾', '更好', '的', '便利', '利生', '生活', '。']]\n",
      "\n",
      " [精確模式]\n",
      "[['人工智慧', '亦', '稱智械', '、', '機器', '智慧', ',', '指由人', '製造', '出來', '的', '機器', '所', '表現', '出来', '的', '智慧', '。'], ['通常', '人工智慧', '是', '指', '透過', '普通', '電腦程式', '來现', '人题', '智慧', '的', '技術', '。'], ['中華', '郵政', '未来', '智慧', '物流', '服務', ',', '將取', '之大眾', '智慧', ',', '中華', '郵政', '帶給', '民眾', '更好', '的', '便利', '生活', '。']]\n",
      "\n",
      " [搜尋引擎模式]\n",
      "[['人工', '智慧', '人工智慧', '亦', '稱智械', '、', '機器', '智慧', ',', '指由人', '製造', '出來', '的', '機器', '所', '表現', '出来', '的', '智慧', '。'], ['通常', '人工', '智慧', '人工智慧', '是', '指', '透過', '普通', '電腦', '程式', '電腦程式', '來现', '人题', '智慧', '的', '技術', '。'], ['中華', '郵政', '未来', '智慧', '物流', '服務', ',', '將取', '之大眾', '智慧', ',', '中華', '郵政', '帶給', '民眾', '更好', '的', '便利', '生活', '。']]\n"
     ]
    }
   ],
   "source": [
    "print(\" [Paddle 模式]\")\n",
    "J_sents_annotated_ws = [jieba.lcut(sent, use_paddle = True) for sent in zh_text]  #使用深度學習去斷詞\n",
    "print(J_sents_annotated_ws)       \n",
    "\n",
    "print(\"\\n [全模式]\")\n",
    "J_sents_annotated_ws = [jieba.lcut(sent, cut_all = True) for sent in zh_text]  \n",
    "print(J_sents_annotated_ws)     \n",
    "\n",
    "print(\"\\n [精確模式]\")\n",
    "J_sents_annotated_ws = [jieba.lcut(sent, cut_all = False) for sent in zh_text]  \n",
    "print(J_sents_annotated_ws)     \n",
    "\n",
    "print(\"\\n [搜尋引擎模式]\")   #切出可能可以用來搜尋的單詞\n",
    "J_sents_annotated_ws = [jieba.lcut_for_search(sent) for sent in zh_text]  \n",
    "print(J_sents_annotated_ws)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [精確模式]\n",
      "[['人工智慧', '亦', '稱智械', '、', '機器', '智慧', ',', '指由人', '製造', '出來', '的', '機器', '所', '表現', '出来', '的', '智慧', '。'], ['通常', '人工智慧', '是', '指', '透過', '普通', '電腦程式', '來现', '人题', '智慧', '的', '技術', '。'], ['中華郵政', '未来', '智慧', '物流', '服務', ',', '將取', '之大眾', '智慧', ',', '中華郵政', '帶給', '民眾', '更好', '的', '便利', '生活', '。']]\n"
     ]
    }
   ],
   "source": [
    "#調整字典\n",
    "jieba.add_word(\"中華郵政\",freq=None, tag=None)\n",
    "\n",
    "print(\"\\n [精確模式]\")\n",
    "J_sents_annotated_ws = [jieba.lcut(sent, cut_all = False) for sent in zh_text]  \n",
    "print(J_sents_annotated_ws) \n",
    "\n",
    "# 若之後不想要，可以使用del_word\n",
    "# jieba.del_word('中華郵政')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 詞性標註"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paddle 模式]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[pair('人工智慧', 'l'),\n",
       "  pair('亦', 'd'),\n",
       "  pair('稱', 'v'),\n",
       "  pair('智械', 'n'),\n",
       "  pair('、', 'x'),\n",
       "  pair('機器', 'x'),\n",
       "  pair('智慧', 'nr'),\n",
       "  pair(',', 'x'),\n",
       "  pair('指由人', 'n'),\n",
       "  pair('製造', 'x'),\n",
       "  pair('出來', 'x'),\n",
       "  pair('的', 'uj'),\n",
       "  pair('機器', 'x'),\n",
       "  pair('所', 'c'),\n",
       "  pair('表現', 'x'),\n",
       "  pair('出来', 'v'),\n",
       "  pair('的', 'uj'),\n",
       "  pair('智慧', 'nr'),\n",
       "  pair('。', 'x')],\n",
       " [pair('通常', 'd'),\n",
       "  pair('人工智慧', 'l'),\n",
       "  pair('是', 'v'),\n",
       "  pair('指', 'n'),\n",
       "  pair('透過', 'x'),\n",
       "  pair('普通', 'nz'),\n",
       "  pair('電腦程式', 'x'),\n",
       "  pair('來现', 'v'),\n",
       "  pair('人题', 'n'),\n",
       "  pair('智慧', 'nr'),\n",
       "  pair('的', 'uj'),\n",
       "  pair('技術', 'x'),\n",
       "  pair('。', 'x')],\n",
       " [pair('中華郵政', 'x'),\n",
       "  pair('未来', 't'),\n",
       "  pair('智慧', 'nr'),\n",
       "  pair('物流', 'n'),\n",
       "  pair('服務', 'x'),\n",
       "  pair(',', 'x'),\n",
       "  pair('將', 'd'),\n",
       "  pair('取', 'v'),\n",
       "  pair('之', 'u'),\n",
       "  pair('大眾', 'n'),\n",
       "  pair('智慧', 'nr'),\n",
       "  pair(',', 'x'),\n",
       "  pair('中華郵政', 'x'),\n",
       "  pair('帶給', 'x'),\n",
       "  pair('民眾', 'n'),\n",
       "  pair('更好', 'd'),\n",
       "  pair('的', 'uj'),\n",
       "  pair('便利', 'a'),\n",
       "  pair('生活', 'vn'),\n",
       "  pair('。', 'x')]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('[paddle 模式]')\n",
    "J_sents_annotated_pos = [pseg.lcut(sent, use_paddle=True) for sent in zh_text]\n",
    "J_sents_annotated_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX\tTEXT\tPOS\n",
      "0\t人工智慧\tl\n",
      "1\t亦\td\n",
      "2\t稱\tv\n",
      "3\t智械\tn\n",
      "4\t、\tx\n",
      "5\t機器\tx\n",
      "6\t智慧\tnr\n",
      "7\t,\tx\n",
      "8\t指由人\tn\n",
      "9\t製造\tx\n",
      "10\t出來\tx\n",
      "11\t的\tuj\n",
      "12\t機器\tx\n",
      "13\t所\tc\n",
      "14\t表現\tx\n",
      "15\t出来\tv\n",
      "16\t的\tuj\n",
      "17\t智慧\tnr\n",
      "18\t。\tx\n",
      "\n",
      "0\t通常\td\n",
      "1\t人工智慧\tl\n",
      "2\t是\tv\n",
      "3\t指\tn\n",
      "4\t透過\tx\n",
      "5\t普通\tnz\n",
      "6\t電腦程式\tx\n",
      "7\t來现\tv\n",
      "8\t人题\tn\n",
      "9\t智慧\tnr\n",
      "10\t的\tuj\n",
      "11\t技術\tx\n",
      "12\t。\tx\n",
      "\n",
      "0\t中華郵政\tx\n",
      "1\t未来\tt\n",
      "2\t智慧\tnr\n",
      "3\t物流\tn\n",
      "4\t服務\tx\n",
      "5\t,\tx\n",
      "6\t將\td\n",
      "7\t取\tv\n",
      "8\t之\tu\n",
      "9\t大眾\tn\n",
      "10\t智慧\tnr\n",
      "11\t,\tx\n",
      "12\t中華郵政\tx\n",
      "13\t帶給\tx\n",
      "14\t民眾\tn\n",
      "15\t更好\td\n",
      "16\t的\tuj\n",
      "17\t便利\ta\n",
      "18\t生活\tvn\n",
      "19\t。\tx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 重新排版\n",
    "\n",
    "print('INDEX\\tTEXT\\tPOS')\n",
    "for sent in J_sents_annotated_pos:\n",
    "    for i,(word,pos) in enumerate(sent):\n",
    "        print(\"{}\\t{}\\t{}\".format(i,word,pos))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Stanza 套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7954415277054bc7b2f95bd21236e595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 13:52:09 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d25b84194b43d88ec587fd6cf94f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 13:53:53 INFO: Finished downloading models and saved to C:\\Users\\yifun\\stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc96d27b0f342eca426d46d1f7b52a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 13:53:54 INFO: Downloading default packages for language: zh-hant (Traditional_Chinese) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab6ed33ccd847338022ec2cfc71298c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-zh-hant/resolve/v1.6.0/models/default.zip:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 13:54:26 INFO: Finished downloading models and saved to C:\\Users\\yifun\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')   # download English model\n",
    "stanza.download('zh-hant')  # download Chinese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 13:55:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127c13becd1b49918561db37764339f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 13:55:32 INFO: Loading these models for language: zh-hant (Traditional_Chinese):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| pos       | gsd_nocharlm |\n",
      "| lemma     | gsd_nocharlm |\n",
      "| depparse  | gsd_nocharlm |\n",
      "============================\n",
      "\n",
      "2023-10-19 13:55:32 WARNING: GPU requested, but is not available!\n",
      "2023-10-19 13:55:32 INFO: Using device: cpu\n",
      "2023-10-19 13:55:32 INFO: Loading: tokenize\n",
      "2023-10-19 13:55:33 INFO: Loading: pos\n",
      "2023-10-19 13:55:33 INFO: Loading: lemma\n",
      "2023-10-19 13:55:33 INFO: Loading: depparse\n",
      "2023-10-19 13:55:33 INFO: Done loading processors!\n",
      "2023-10-19 13:55:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d31befad3f84470a3386bb0e1f86404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 13:55:34 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2023-10-19 13:55:34 WARNING: GPU requested, but is not available!\n",
      "2023-10-19 13:55:34 INFO: Using device: cpu\n",
      "2023-10-19 13:55:34 INFO: Loading: tokenize\n",
      "2023-10-19 13:55:34 INFO: Loading: pos\n",
      "2023-10-19 13:55:34 INFO: Loading: lemma\n",
      "2023-10-19 13:55:34 INFO: Loading: depparse\n",
      "2023-10-19 13:55:34 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Initialize Chinese annotator\n",
    "stanza_zh_annotator = stanza.Pipeline(lang='zh-hant',processors='tokenize,lemma,pos,depparse',use_gpu=True)\n",
    "                                            # 這邊要告訴標註器要做: 斷詞、還原字根、詞性標註、結構樹     #用GPU來加速\n",
    "\n",
    "#Initialize English annotator\n",
    "stanza_en_annotator = stanza.Pipeline(lang='en',processors='tokenize,lemma,pos,depparse',use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斷詞、詞性標註、句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(doc):\n",
    "    for i, sent in enumerate(doc.sentences):  # 讀出每一句話\n",
    "        print(\"Sentence {}\\nINDEX\\tTEXT\\tLEMMA\\tPOS\\tDepRel\\tHeadId\".format(i+1))\n",
    "        for j, word in enumerate(sent.words):  # 將每句話的每個字，依序顯示出來\n",
    "            print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(j,word.text,word.lemma,word.pos,word.deprel,word.head))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_sents_annotated_ws_pos_deprel_zh:\n",
      "Sentence 1\n",
      "INDEX\tTEXT\tLEMMA\tPOS\tDepRel\tHeadId\n",
      "0\t人工\t人工\tNOUN\tnmod\t2\n",
      "1\t智慧\t智慧\tNOUN\tnsubj\t4\n",
      "2\t亦\t亦\tADV\tmark\t4\n",
      "3\t稱\t稱\tVERB\troot\t0\n",
      "4\t智械\t智械\tNOUN\tobj\t4\n",
      "5\t、\t、\tPUNCT\tpunct\t8\n",
      "6\t機器\t機器\tNOUN\tnmod\t8\n",
      "7\t智慧\t智慧\tNOUN\tconj\t5\n",
      "8\t,\t,\tADV\tadvmod\t10\n",
      "9\t指\t指\tVERB\tccomp\t4\n",
      "10\t由\t由\tADP\tcase\t12\n",
      "11\t人\t人\tNOUN\tobl\t13\n",
      "12\t製造\t製造\tVERB\tacl:relcl\t16\n",
      "13\t出來\t出來\tVERB\tmark\t13\n",
      "14\t的\t的\tPART\tmark:rel\t13\n",
      "15\t機器\t機器\tNOUN\tnsubj\t18\n",
      "16\t所\t所\tADV\tmark\t18\n",
      "17\t表現\t表現\tVERB\tacl:relcl\t21\n",
      "18\t出来\t出来\tVERB\tacl:relcl\t21\n",
      "19\t的\t的\tPART\tmark:rel\t19\n",
      "20\t智慧\t智慧\tNOUN\tobj\t10\n",
      "21\t。\t。\tPUNCT\tpunct\t10\n",
      "\n",
      "Sentence 1\n",
      "INDEX\tTEXT\tLEMMA\tPOS\tDepRel\tHeadId\n",
      "0\t通常\t通常\tADJ\tamod\t3\n",
      "1\t人工\t人工\tNOUN\tnmod\t3\n",
      "2\t智慧\t智慧\tNOUN\tnsubj\t4\n",
      "3\t是\t是\tVERB\troot\t0\n",
      "4\t指\t指\tVERB\txcomp\t4\n",
      "5\t透過\t透過\tADP\tcase\t9\n",
      "6\t普通\t普通\tADJ\tamod\t9\n",
      "7\t電腦\t電腦\tNOUN\tnmod\t9\n",
      "8\t程式\t程式\tNOUN\tobl\t11\n",
      "9\t來\t來\tADV\tmark\t11\n",
      "10\t现\t现\tVERB\txcomp\t5\n",
      "11\t人\t人\tNOUN\tnsubj\t13\n",
      "12\t题\t题\tVERB\tacl:relcl\t16\n",
      "13\t智慧\t智慧\tNOUN\tobj\t13\n",
      "14\t的\t的\tPART\tmark:rel\t13\n",
      "15\t技術\t技術\tNOUN\tobj\t5\n",
      "16\t。\t。\tPUNCT\tpunct\t4\n",
      "\n",
      "Sentence 1\n",
      "INDEX\tTEXT\tLEMMA\tPOS\tDepRel\tHeadId\n",
      "0\t中華\t中華\tPROPN\tnmod\t2\n",
      "1\t郵政\t郵政\tNOUN\tnsubj\t10\n",
      "2\t未\t未\tADV\tadvmod\t4\n",
      "3\t来\t来\tVERB\tadvcl\t10\n",
      "4\t智慧\t智慧\tNOUN\tnmod\t7\n",
      "5\t物流\t物流\tNOUN\tnmod\t7\n",
      "6\t服務\t服務\tNOUN\tobj\t4\n",
      "7\t,\t,\tADV\tmark\t10\n",
      "8\t將\t將\tADV\tadvmod\t10\n",
      "9\t取之\t取之\tVERB\troot\t0\n",
      "10\t大眾\t大眾\tNOUN\tnmod\t12\n",
      "11\t智慧\t智慧\tNOUN\tnsubj\t16\n",
      "12\t,\t,\tADP\tcase\t15\n",
      "13\t中華\t中華\tPROPN\tnmod\t15\n",
      "14\t郵政\t郵政\tNOUN\tobl\t16\n",
      "15\t帶給\t帶給\tVERB\tccomp\t10\n",
      "16\t民眾\t民眾\tNOUN\tnsubj\t18\n",
      "17\t更好\t更好\tADJ\tacl:relcl\t21\n",
      "18\t的\t的\tPART\tmark:rel\t18\n",
      "19\t便利\t便利\tADJ\tamod\t21\n",
      "20\t生活\t生活\tNOUN\tobj\t16\n",
      "21\t。\t。\tPUNCT\tpunct\t10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用中文斷詞器進行分析\n",
    "print(\"S_sents_annotated_ws_pos_deprel_zh:\")\n",
    "S_sents_annotated_ws_pos_deprel_zh = [stanza_zh_annotator(sent) for sent in zh_text]\n",
    "for sent in S_sents_annotated_ws_pos_deprel_zh:\n",
    "    show(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_sents_annotated_ws_pos_deprel_en:\n",
      "Sentence 1\n",
      "INDEX\tTEXT\tLEMMA\tPOS\tDepRel\tHeadId\n",
      "0\tIn\tin\tADP\tcase\t2\n",
      "1\t1951\t1951\tNUM\tobl\t9\n",
      "2\t,\t,\tPUNCT\tpunct\t9\n",
      "3\ta\ta\tDET\tdet\t5\n",
      "4\tboard\tboard\tNOUN\tnsubj:pass\t9\n",
      "5\tof\tof\tADP\tcase\t7\n",
      "6\tdirectors\tdirector\tNOUN\tnmod\t5\n",
      "7\twas\tbe\tAUX\taux:pass\t9\n",
      "8\tcreated\tcreate\tVERB\troot\t0\n",
      "9\tand\tand\tCCONJ\tcc\t13\n",
      "10\tpremises\tpremise\tNOUN\tnsubj:pass\t13\n",
      "11\twere\tbe\tAUX\taux:pass\t13\n",
      "12\trented\trent\tVERB\tconj\t9\n",
      "13\ton\ton\tADP\tcase\t16\n",
      "14\tHankou\tHankou\tPROPN\tcompound\t16\n",
      "15\tStreet\tStreet\tPROPN\tobl\t13\n",
      "16\tin\tin\tADP\tcase\t19\n",
      "17\tdowntown\tdowntown\tADJ\tamod\t19\n",
      "18\tTaipe\tTaipe\tPROPN\tnmod\t16\n",
      "19\t;\t;\tPUNCT\tpunct\t22\n",
      "20\tto\tto\tPART\tmark\t22\n",
      "21\tset\tset\tVERB\tadvcl\t13\n",
      "22\tup\tup\tADP\tcompound:prt\t22\n",
      "23\tthe\tthe\tDET\tdet\t30\n",
      "24\tso\tso\tADV\tadvmod\t27\n",
      "25\t-\t-\tPUNCT\tpunct\t27\n",
      "26\tcalled\tcall\tVERB\tamod\t30\n",
      "27\tSoochow\tSoochow\tPROPN\tcompound\t30\n",
      "28\tPreparatory\tPreparatory\tADJ\tamod\t30\n",
      "29\tSchoo\tSchoo\tPROPN\tobj\t22\n",
      "30\t.\t.\tPUNCT\tpunct\t9\n",
      "\n",
      "Sentence 1\n",
      "INDEX\tTEXT\tLEMMA\tPOS\tDepRel\tHeadId\n",
      "0\tThe\tthe\tDET\tdet\t2\n",
      "1\tschool\tschool\tNOUN\tnsubj\t3\n",
      "2\tbecame\tbecome\tVERB\troot\t0\n",
      "3\tthe\tthe\tDET\tdet\t7\n",
      "4\tfirst\tfirst\tADJ\tamod\t7\n",
      "5\tprivate\tprivate\tADJ\tamod\t7\n",
      "6\tuniversity\tuniversity\tNOUN\txcomp\t3\n",
      "7\tin\tin\tADP\tcase\t9\n",
      "8\tTaiwan\tTaiwan\tPROPN\tnmod\t7\n",
      "9\t.\t.\tPUNCT\tpunct\t3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用英文斷詞器進行分析\n",
    "print(\"S_sents_annotated_ws_pos_deprel_en:\")\n",
    "S_sents_annotated_ws_pos_deprel_en = [stanza_en_annotator(sent) for sent in en_text]\n",
    "for sent in S_sents_annotated_ws_pos_deprel_en:\n",
    "    show(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 命名實體辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 14:20:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e92bf42bd84ea68d29a1c8e9bafe28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 14:20:56 INFO: Loading these models for language: en (English):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | combined         |\n",
      "| ner       | ontonotes_charlm |\n",
      "================================\n",
      "\n",
      "2023-10-19 14:20:56 WARNING: GPU requested, but is not available!\n",
      "2023-10-19 14:20:56 INFO: Using device: cpu\n",
      "2023-10-19 14:20:56 INFO: Loading: tokenize\n",
      "2023-10-19 14:20:56 INFO: Loading: ner\n",
      "2023-10-19 14:20:57 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize English annotator for NER\n",
    "stanza_en_annotator = stanza.Pipeline(lang='en',processors=\"tokenize,ner\",use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ner(doc):\n",
    "    for i, sent in enumerate(doc.sentences):  # 讀出每一句話\n",
    "        print(\"Sentence {}\\nID\\tTEXT\\tNER\".format(i+1))\n",
    "        for j, token in enumerate(sent.tokens):  # 將每句話的每個字，依序顯示出來\n",
    "            print(\"{}\\t{}\\t{}\".format(j,token.text,token.ner))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_sents_annotated_NER_en:\n",
      "Sentence 1\n",
      "ID\tTEXT\tNER\n",
      "0\tIn\tO\n",
      "1\t1951\tS-DATE\n",
      "2\t,\tO\n",
      "3\ta\tO\n",
      "4\tboard\tO\n",
      "5\tof\tO\n",
      "6\tdirectors\tO\n",
      "7\twas\tO\n",
      "8\tcreated\tO\n",
      "9\tand\tO\n",
      "10\tpremises\tO\n",
      "11\twere\tO\n",
      "12\trented\tO\n",
      "13\ton\tO\n",
      "14\tHankou\tB-FAC\n",
      "15\tStreet\tE-FAC\n",
      "16\tin\tO\n",
      "17\tdowntown\tO\n",
      "18\tTaipe\tS-GPE\n",
      "19\t;\tO\n",
      "20\tto\tO\n",
      "21\tset\tO\n",
      "22\tup\tO\n",
      "23\tthe\tO\n",
      "24\tso\tO\n",
      "25\t-\tO\n",
      "26\tcalled\tO\n",
      "27\tSoochow\tB-ORG\n",
      "28\tPreparatory\tI-ORG\n",
      "29\tSchoo\tE-ORG\n",
      "30\t.\tO\n",
      "\n",
      "Sentence 1\n",
      "ID\tTEXT\tNER\n",
      "0\tThe\tO\n",
      "1\tschool\tO\n",
      "2\tbecame\tO\n",
      "3\tthe\tO\n",
      "4\tfirst\tS-ORDINAL\n",
      "5\tprivate\tO\n",
      "6\tuniversity\tO\n",
      "7\tin\tO\n",
      "8\tTaiwan\tS-GPE\n",
      "9\t.\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用英文斷詞器進行分析\n",
    "print(\"S_sents_annotated_NER_en:\")\n",
    "S_sents_annotated_NER_en = [stanza_en_annotator(sent) for sent in en_text]\n",
    "for sent in S_sents_annotated_NER_en:\n",
    "    show_ner(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
