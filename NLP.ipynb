{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用斷詞套件，Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.set_dictionary('dict.txt.big')   #載入繁體辭典"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jieba 有四種斷詞模式:\n",
    "1. 精確模式:     基本款\n",
    "\n",
    "2. 全模式:       將cut_all設為true，會將可以斷的詞再繼續斷開，列出所有可能的詞\n",
    "\n",
    "3. paddle模式:   Jieba導入一個深度學習框架PaddlePaddle來進行分詞，Paddle模式相比传统的基于规则的分词算法，在一些特定的场\n",
    "                景下可能会有更好的效果。據說是利用GRU(循環神經網路,Gate Recurrent Unit) ，必須要先利用 \n",
    "                jieba.enable_paddle()來啟動。\n",
    "\n",
    "4. 搜尋引擎模式: 這個模式跟全模式很像，都可以將詞再更切分\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精確模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精確模式: 我/ 愛/ 自然語言/ 處理\n"
     ]
    }
   ],
   "source": [
    "text_after_jieba = jieba.cut(\"我愛自然語言處理\", cut_all=False) # cut_all=False 為精確模式\n",
    "print(\"精確模式: \" + \"/ \".join(text_after_jieba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x000001900E05F970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_after_jieba  # 在使用jieba分詞後打印text_after_jieba沒有任何內容，可能是因為jieba.cut返回的是一個生成器（generator）\n",
    "                  #，而不是一個列表或字符串。生成器是一種特殊的對象，它需要進行迭代才能獲取其中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n",
      "愛\n",
      "自然語言\n",
      "處理\n"
     ]
    }
   ],
   "source": [
    "text_after_jieba = jieba.cut(\"我愛自然語言處理\", cut_all=False)\n",
    "for i in text_after_jieba:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式: 我/ 愛/ 自然/ 自然語言/ 語言/ 處理\n"
     ]
    }
   ],
   "source": [
    "text_after_jieba = jieba.cut(\"我愛自然語言處理\", cut_all=True) # 全模式\n",
    "print(\"全模式: \" + \"/ \".join(text_after_jieba))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paddle模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle: 我/ 愛/ 自然/ 語言/ 處理\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "paddle.enable_static()  # 進入靜態圖模式\n",
    "jieba.enable_paddle()\n",
    "seg_list = jieba.cut(\"我愛自然語言處理\", use_paddle=True)  # 使用Paddle模式\n",
    "print(\"paddle: \" + \"/ \".join(seg_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜尋引擎模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜尋引擎: 我/愛/自然/語言/自然語言/處理\n"
     ]
    }
   ],
   "source": [
    "text_after_jieba = jieba.cut_for_search('我愛自然語言處理')\n",
    "print(\"搜尋引擎: \" + \"/\".join(text_after_jieba))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 辨識新字詞\n",
    "\n",
    "若Jieba遇到新詞，可以透過HMM( Hidden Markov Models）來預測 找出『未登錄詞』\n",
    "\n",
    "HMM 模型就是一種統計斷詞的方法，未登錄詞就是想成詞典中沒有這種詞，必須經過統計來得知新的詞語。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM output:['我', '今天', '想', '吃', '東西']\n"
     ]
    }
   ],
   "source": [
    "words = jieba.cut(\"我今天想吃東西\", HMM=True)\n",
    "words_list = []\n",
    "for word in words:\n",
    "    words_list.append(word)\n",
    "\n",
    "print(f\"HMM output:{words_list}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入自定義詞典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:['NBA', '金洲', '勇士隊', '2023', '總冠軍']\n",
      "output:['NBA', '金洲勇士', '隊', '2023', '總冠軍']\n"
     ]
    }
   ],
   "source": [
    "text = 'NBA金洲勇士隊2023總冠軍'\n",
    "words = jieba.cut(text)\n",
    "words_list = []\n",
    "for word in words:\n",
    "    words_list.append(word)\n",
    "\n",
    "print(f'output:{words_list}')\n",
    "\n",
    "jieba.load_userdict(r\"C:\\Users\\yifun\\Desktop\\python\\define.txt.txt\")   #載入我自己定義的詞典\n",
    "\n",
    "words = jieba.cut(text)\n",
    "words_list = []\n",
    "for word in words:\n",
    "    words_list.append(word)\n",
    "\n",
    "print(f'output:{words_list}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pos tagging( 詞性標註 , Part-of-speech tagging) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "愛 v\n",
      "自然語言 l\n",
      "處理 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "words = jieba.posseg.cut('我愛自然語言處理')\n",
    "for word, flag in words:\n",
    "    print(f'{word} {flag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('welcome', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('Categorizing', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagging', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Python', 'NNP')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常見的方法:\n",
    "\n",
    "- Rule-Based：自訂 rules 來標記單詞，如看到 ed、i，或是看到'看'、'打'就標注 verb。\n",
    "- Probabilistic：使用條件機率的原理，預測單詞詞性，常見如 CRF、HMM，此方法也是深度學習出來前，最常見且效果最好的標注方式。\n",
    "- Deep Learning：使用深度學習模型預測標註詞性，例如使用LSTM, BERT等方法進行多對多的訓練，這裡有很多相關的資訊可以參考"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本/詞表示方式\n",
    "- BOW(Bag of word)詞袋\n",
    "- one-hot represtation\n",
    "- tf-idf  ('Term Frequency(詞頻)'與 'Inverted Document Frequency(逆詞頻)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import math\n",
    "\n",
    "# 載入繁體\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "\n",
    "text_a = '從GPT-3衍生改良而來的Codex模型，能夠將使用者的自然語言指令轉換為程式碼，OpenAI現在以私人測試的方式釋出CodexAPI'\n",
    "text_b = 'Blender2.0除了能即時搜尋網路資訊，臉書也為其打造新的神經模組，可根據之前使用者與它的聊天脈絡來累積記憶'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始斷詞與紀錄每個詞出現的次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'模型': 1, '為': 1, '能夠': 1, '資訊': 0, '記憶': 0, '可': 0, '方式': 1, '而來': 1, '除了': 0, 'GPT': 1, '指令': 1, '也': 0, '打造': 0, '以': 1, '程式碼': 1, '現在': 1, 'CodexAPI': 1, '搜尋': 0, 'Codex': 1, '模組': 0, '根據': 0, '累積': 0, '新': 0, '私人': 1, '，': 2, '與': 0, '改良': 1, '從': 1, '它': 0, '即時': 0, '-': 1, '來': 0, '3': 1, 'Blender2.0': 0, '衍生': 1, '臉書': 0, '釋出': 1, '能': 0, '轉換': 1, '將': 1, '脈絡': 0, '自然語言': 1, 'OpenAI': 1, '聊天': 0, '測試': 1, '之前': 0, '神經': 0, '其': 0, '網路': 0, '使用者': 1, '的': 3}\n",
      "{'模型': 0, '為': 1, '能夠': 0, '資訊': 1, '記憶': 1, '可': 1, '方式': 0, '而來': 0, '除了': 1, 'GPT': 0, '指令': 0, '也': 1, '打造': 1, '以': 0, '程式碼': 0, '現在': 0, 'CodexAPI': 0, '搜尋': 1, 'Codex': 0, '模組': 1, '根據': 1, '累積': 1, '新': 1, '私人': 0, '，': 2, '與': 1, '改良': 0, '從': 0, '它': 1, '即時': 1, '-': 0, '來': 1, '3': 0, 'Blender2.0': 1, '衍生': 0, '臉書': 1, '釋出': 0, '能': 1, '轉換': 0, '將': 0, '脈絡': 1, '自然語言': 0, 'OpenAI': 0, '聊天': 1, '測試': 0, '之前': 1, '神經': 1, '其': 1, '網路': 1, '使用者': 1, '的': 2}\n"
     ]
    }
   ],
   "source": [
    "#斷詞\n",
    "seg_1 =jieba.lcut(text_a)\n",
    "seg_2 = jieba.lcut(text_b)\n",
    "unique_words = set(seg_1).union(set(seg_2))   #取聯集，取得所有文件中的單詞\n",
    "#建立兩個新字典，分別存兩篇文章詞的出現次數\n",
    "num_words_1 = dict.fromkeys(unique_words,0)     #用來記錄第一個文章在全部的詞中，有哪些出現，並累積次數，從0開始\n",
    "num_words_2 = dict.fromkeys(unique_words,0)     #   記錄第二篇文章 ~\n",
    "\n",
    "for i in seg_1:\n",
    "    num_words_1[i] += 1\n",
    "\n",
    "for j in seg_2:\n",
    "    num_words_2[j] += 1\n",
    "\n",
    "print(num_words_1)\n",
    "print(num_words_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實作TF與IDF的function\n",
    "\n",
    "- 詞頻（TF）=某個詞在文檔中出現的次數/文檔的總詞數\n",
    "\n",
    "- 逆文檔頻率（IDF）=log(語料庫的文檔總數/(包含該詞的文檔數+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF_value(w_dict, text_seg_len):\n",
    "    tf_dict = {}\n",
    "    \n",
    "    for w, count in w_dict.items():\n",
    "        # 計算tf的公式\n",
    "        tf_dict[w] = count / float(text_seg_len)\n",
    "    \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDF_value(text_list, all_words):\n",
    "    \n",
    "    idf_dict = dict.fromkeys(all_words.keys(), 0)\n",
    "    \n",
    "    for text in text_list:\n",
    "        for w, val in text.items():\n",
    "            # 表示出現過在一次文本中         \n",
    "            if val > 0:\n",
    "                idf_dict[w] += 1\n",
    "    \n",
    "    for w, val in idf_dict.items():\n",
    "        # 計算idf的公式\n",
    "        idf_dict[w] = math.log(len(text_list) / float(val))\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_1 = get_TF_value(num_words_1, len(seg_1))\n",
    "tf_2 = get_TF_value(num_words_2, len(seg_2))\n",
    "\n",
    "idf = get_IDF_value([num_words_1, num_words_2], num_words_1)\n",
    "\n",
    "# 計算tfidf\n",
    "tfidf_1 = {}\n",
    "tfidf_2 = {}\n",
    "for w, val in tf_1.items():\n",
    "    tfidf_1[w] = val * idf[w]\n",
    "\n",
    "for w, val in tf_2.items():\n",
    "    tfidf_2[w] = val * idf[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'模型': 0.023104906018664842,\n",
       " '為': 0.0,\n",
       " '能夠': 0.023104906018664842,\n",
       " '資訊': 0.0,\n",
       " '記憶': 0.0,\n",
       " '可': 0.0,\n",
       " '方式': 0.023104906018664842,\n",
       " '而來': 0.023104906018664842,\n",
       " '除了': 0.0,\n",
       " 'GPT': 0.023104906018664842,\n",
       " '指令': 0.023104906018664842,\n",
       " '也': 0.0,\n",
       " '打造': 0.0,\n",
       " '以': 0.023104906018664842,\n",
       " '程式碼': 0.023104906018664842,\n",
       " '現在': 0.023104906018664842,\n",
       " 'CodexAPI': 0.023104906018664842,\n",
       " '搜尋': 0.0,\n",
       " 'Codex': 0.023104906018664842,\n",
       " '模組': 0.0,\n",
       " '根據': 0.0,\n",
       " '累積': 0.0,\n",
       " '新': 0.0,\n",
       " '私人': 0.023104906018664842,\n",
       " '，': 0.0,\n",
       " '與': 0.0,\n",
       " '改良': 0.023104906018664842,\n",
       " '從': 0.023104906018664842,\n",
       " '它': 0.0,\n",
       " '即時': 0.0,\n",
       " '-': 0.023104906018664842,\n",
       " '來': 0.0,\n",
       " '3': 0.023104906018664842,\n",
       " 'Blender2.0': 0.0,\n",
       " '衍生': 0.023104906018664842,\n",
       " '臉書': 0.0,\n",
       " '釋出': 0.023104906018664842,\n",
       " '能': 0.0,\n",
       " '轉換': 0.023104906018664842,\n",
       " '將': 0.023104906018664842,\n",
       " '脈絡': 0.0,\n",
       " '自然語言': 0.023104906018664842,\n",
       " 'OpenAI': 0.023104906018664842,\n",
       " '聊天': 0.0,\n",
       " '測試': 0.023104906018664842,\n",
       " '之前': 0.0,\n",
       " '神經': 0.0,\n",
       " '其': 0.0,\n",
       " '網路': 0.0,\n",
       " '使用者': 0.0,\n",
       " '的': 0.0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'模型': 0.0,\n",
       " '為': 0.0,\n",
       " '能夠': 0.0,\n",
       " '資訊': 0.023104906018664842,\n",
       " '記憶': 0.023104906018664842,\n",
       " '可': 0.023104906018664842,\n",
       " '方式': 0.0,\n",
       " '而來': 0.0,\n",
       " '除了': 0.023104906018664842,\n",
       " 'GPT': 0.0,\n",
       " '指令': 0.0,\n",
       " '也': 0.023104906018664842,\n",
       " '打造': 0.023104906018664842,\n",
       " '以': 0.0,\n",
       " '程式碼': 0.0,\n",
       " '現在': 0.0,\n",
       " 'CodexAPI': 0.0,\n",
       " '搜尋': 0.023104906018664842,\n",
       " 'Codex': 0.0,\n",
       " '模組': 0.023104906018664842,\n",
       " '根據': 0.023104906018664842,\n",
       " '累積': 0.023104906018664842,\n",
       " '新': 0.023104906018664842,\n",
       " '私人': 0.0,\n",
       " '，': 0.0,\n",
       " '與': 0.023104906018664842,\n",
       " '改良': 0.0,\n",
       " '從': 0.0,\n",
       " '它': 0.023104906018664842,\n",
       " '即時': 0.023104906018664842,\n",
       " '-': 0.0,\n",
       " '來': 0.023104906018664842,\n",
       " '3': 0.0,\n",
       " 'Blender2.0': 0.023104906018664842,\n",
       " '衍生': 0.0,\n",
       " '臉書': 0.023104906018664842,\n",
       " '釋出': 0.0,\n",
       " '能': 0.023104906018664842,\n",
       " '轉換': 0.0,\n",
       " '將': 0.0,\n",
       " '脈絡': 0.023104906018664842,\n",
       " '自然語言': 0.0,\n",
       " 'OpenAI': 0.0,\n",
       " '聊天': 0.023104906018664842,\n",
       " '測試': 0.0,\n",
       " '之前': 0.023104906018664842,\n",
       " '神經': 0.023104906018664842,\n",
       " '其': 0.023104906018664842,\n",
       " '網路': 0.023104906018664842,\n",
       " '使用者': 0.0,\n",
       " '的': 0.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用TF/IDF表示成句字/文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.023104906018664842, 0.0, 0.023104906018664842, 0.0, 0.0, 0.0, 0.023104906018664842, 0.023104906018664842, 0.0, 0.023104906018664842, 0.023104906018664842, 0.0, 0.0, 0.023104906018664842, 0.023104906018664842, 0.023104906018664842, 0.023104906018664842, 0.0, 0.023104906018664842, 0.0, 0.0, 0.0, 0.0, 0.023104906018664842, 0.0, 0.0, 0.023104906018664842, 0.023104906018664842, 0.0, 0.0, 0.023104906018664842, 0.0, 0.023104906018664842, 0.0, 0.023104906018664842, 0.0, 0.023104906018664842, 0.0, 0.023104906018664842, 0.023104906018664842, 0.0, 0.023104906018664842, 0.023104906018664842, 0.0, 0.023104906018664842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 創建一個表示text a的list\n",
    "bow_a = []\n",
    "# 將tfidf_a帶入即可\n",
    "for w, val in tfidf_a.items():\n",
    "    bow_a.append(val)\n",
    "\n",
    "print(bow_a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用CKIP套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yifun\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 269k/269k [00:00<00:00, 480kB/s]\n",
      "C:\\Users\\yifun\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yifun\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 729/729 [00:00<00:00, 111kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 16.2M/16.2M [00:02<00:00, 6.81MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 758/758 [00:00<00:00, 216kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 421M/421M [01:00<00:00, 6.91MB/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 832/832 [00:00<00:00, 381kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 15.9M/15.9M [00:03<00:00, 5.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "   BertTokenizerFast,\n",
    "   AutoModelForMaskedLM,\n",
    "   AutoModelForCausalLM,\n",
    "   AutoModelForTokenClassification,\n",
    ")\n",
    "\n",
    "# masked language model (ALBERT, BERT)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForMaskedLM.from_pretrained('ckiplab/albert-tiny-chinese') # or other models above\n",
    "\n",
    "# casual language model (GPT2)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese') # or other models above\n",
    "\n",
    "# nlp task model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForTokenClassification.from_pretrained('ckiplab/albert-tiny-chinese-ws') # or other models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 804/804 [00:00<00:00, 298kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 407M/407M [01:00<00:00, 6.70MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 301/301 [00:00<00:00, 151kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 311kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 40.4kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 2.86k/2.86k [00:00<00:00, 1.43MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 407M/407M [01:02<00:00, 6.46MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 301/301 [00:00<00:00, 79.3kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 317kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 28.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 3.71k/3.71k [00:00<00:00, 741kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 407M/407M [00:53<00:00, 7.64MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 301/301 [00:00<00:00, 95.5kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 314kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 28.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize drivers\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"bert-base\")\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU\n",
    "ws_driver = CkipWordSegmenter(device=-1)\n",
    "\n",
    "# Use GPU:0\n",
    "ws_driver = CkipWordSegmenter(device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 2077.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 2998.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<?, ?it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = [\n",
    "   \"傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。\",\n",
    "   \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。\",\n",
    "   \"空白 也是可以的～\",\n",
    "]\n",
    "\n",
    "# Run pipeline\n",
    "ws  = ws_driver(text)\n",
    "pos = pos_driver(ws)\n",
    "ner = ner_driver(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。\n",
      "傅達仁(Nb)　今(Nd)　將(D)　執行(VC)　安樂死(Na)　，(COMMACATEGORY)　卻(D)　突然(D)　爆出(VJ)　自己(Nh)　20(Neu)　年(Nf)　前(Ng)　遭(P)　緯來(Nb)　體育台(Na)　封殺(VC)　，(COMMACATEGORY)　他(Nh)　不(D)　懂(VK)　自己(Nh)　哪裡(Ncd)　得罪到(VC)　電視台(Nc)　。(PERIODCATEGORY)\n",
      "NerToken(word='傅達仁', ner='PERSON', idx=(0, 3))\n",
      "NerToken(word='20年', ner='DATE', idx=(18, 21))\n",
      "NerToken(word='緯來體育台', ner='ORG', idx=(23, 28))\n",
      "\n",
      "美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。\n",
      "美國(Nc)　參議院(Nc)　針對(P)　今天(Nd)　總統(Na)　布什(Nb)　所(D)　提名(VC)　的(DE)　勞工部長(Na)　趙小蘭(Nb)　展開(VC)　認可(VC)　聽證會(Na)　，(COMMACATEGORY)　預料(VE)　她(Nh)　將(D)　會(D)　很(Dfa)　順利(VH)　通過(VC)　參議院(Nc)　支持(Nv)　，(COMMACATEGORY)　成為(VG)　該(Nes)　國(Nc)　有史以來(D)　第一(Neu)　位(Nf)　的(DE)　華裔(Na)　女性(Na)　內閣(Na)　成員(Na)　。(PERIODCATEGORY)\n",
      "NerToken(word='美國參議院', ner='ORG', idx=(0, 5))\n",
      "NerToken(word='今天', ner='LOC', idx=(7, 9))\n",
      "NerToken(word='布什', ner='PERSON', idx=(11, 13))\n",
      "NerToken(word='勞工部長', ner='ORG', idx=(17, 21))\n",
      "NerToken(word='趙小蘭', ner='PERSON', idx=(21, 24))\n",
      "NerToken(word='認可聽證會', ner='EVENT', idx=(26, 31))\n",
      "NerToken(word='參議院', ner='ORG', idx=(42, 45))\n",
      "NerToken(word='第一', ner='ORDINAL', idx=(56, 58))\n",
      "NerToken(word='華裔', ner='NORP', idx=(60, 62))\n",
      "\n",
      "空白 也是可以的～\n",
      "空白(VH)　 (WHITESPACE)　也(D)　是(SHI)　可以(VH)　的(T)　～(FW)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pack word segmentation and part-of-speech results\n",
    "def pack_ws_pos_sentece(sentence_ws, sentence_pos):\n",
    "   assert len(sentence_ws) == len(sentence_pos)\n",
    "   res = []\n",
    "   for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "      res.append(f\"{word_ws}({word_pos})\")\n",
    "   return \"\\u3000\".join(res)\n",
    "\n",
    "# Show results\n",
    "for sentence, sentence_ws, sentence_pos, sentence_ner in zip(text, ws, pos, ner):\n",
    "   print(sentence)\n",
    "   print(pack_ws_pos_sentece(sentence_ws, sentence_pos))\n",
    "   for entity in sentence_ner:\n",
    "      print(entity)\n",
    "   print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
